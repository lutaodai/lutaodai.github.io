<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Lutao Dai">
    
    <title>
        
            An Overview of Deep Learning for Curious People |
        
        Lutao&#39;s Blog
    </title>
    <link rel="shortcut icon" href="/images/profile.jpg">
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.xml"};
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true},"style":{"primary_color":"#0066CC","avatar":"/images/profile.jpg","favicon":"/images/profile.jpg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":false,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving."},"scroll":{"progress_bar":{"enable":false},"percent":{"enable":false}}},"local_search":{"enable":true,"trigger":"auto","unescape":false,"preload":true},"code_copy":{"enable":true,"style":"default"},"side_tools":{"enable":true},"pjax":{"enable":false},"lazyload":{"enable":false},"version":"3.3.0"};
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days age","week":"%s weeks age","month":"%s months age","year":"%s years age"};
  </script>
<meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            <a class="logo-title" href="/">
                Lutao&#39;s Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                CATEGORIES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                TAGS
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">CATEGORIES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">TAGS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content normal-code-theme">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">An Overview of Deep Learning for Curious People</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/profile.jpg">
                </div>
                <div class="info">
                    <div class="author">
                        <span>Lutao Dai</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i> 2017-06-21 01:09:00
    </span>
    
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i> <span>2.7k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i> <span>17 Mins</span>
        </span>
    
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <blockquote>
<p>Starting earlier this year, I grew a strong curiosity of deep learning and spent some time reading about this field. To document what I’ve learned and to provide some interesting pointers to people with similar interests, I wrote this overview of deep learning models and their applications.</p>
</blockquote>
<a id="more"></a>

<p>(The post was originated from my talk for <a class="link"   target="_blank" rel="noopener" href="http://wimlds.org/chapters/about-bay-area/" >WiMLDS x Fintech meetup<i class="fas fa-external-link-alt"></i></a> hosted by <a href="www.affirm.com">Affirm</a>.)</p>
<p>{: class=”table-of-content”}</p>
<ul>
<li>TOC<br>{:toc}</li>
</ul>
<hr>
<p>I believe many of you have watched or heard of the <a class="link"   target="_blank" rel="noopener" href="https://youtu.be/vFr3K2DORc8" >games<i class="fas fa-external-link-alt"></i></a> between AlphaGo and professional Go player <a class="link"   target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Lee_Sedol" >Lee Sedol<i class="fas fa-external-link-alt"></i></a> in 2016. Lee has the highest rank of nine dan and many world championships. No doubt, he is one of the best Go players in the world, but he <a class="link"   target="_blank" rel="noopener" href="https://www.scientificamerican.com/article/how-the-computer-beat-the-go-master/" >lost by 1-4<i class="fas fa-external-link-alt"></i></a> in this series versus AlphaGo. Before this, Go was considered to be an intractable game for computers to master, as its simple rules lay out an exponential number of variations in the board positions, many more than what in Chess. This event surely highlighted 2016 as a big year for AI. Because of AlphaGo, much attention has been attracted to the progress of AI.</p>
<p>Meanwhile, many companies are spending resources on pushing the edges of AI applications, that indeed have the potential to change or even revolutionize how we are gonna live. Familiar examples include self-driving cars, chatbots, home assistant devices and many others. One of the secret receipts behind the progress we have had in recent years is deep learning.</p>
<h2 id="Why-Does-Deep-Learning-Work-Now"><a href="#Why-Does-Deep-Learning-Work-Now" class="headerlink" title="Why Does Deep Learning Work Now?"></a>Why Does Deep Learning Work Now?</h2><p>Deep learning models, in simple words, are large and deep artificial neural nets. A neural network (“NN”) can be well presented in a <a class="link"   target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" >directed acyclic graph<i class="fas fa-external-link-alt"></i></a>: the input layer takes in signal vectors; one or multiple hidden layers process the outputs of the previous layer. The initial concept of a neural network can be traced back to more than <a class="link"   target="_blank" rel="noopener" href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html" >half a century ago<i class="fas fa-external-link-alt"></i></a>. But why does it work now? Why do people start talking about them all of a sudden? </p>
<p><img src="!--swig%EF%BF%BC0--" alt="Artificial neural network"><br>{: style=”width: 400px; max-width: 100%;”}<br><em>Fig 1. A three-layer artificial neural network. (Image source: <a class="link"   target="_blank" rel="noopener" href="http://cs231n.github.io/convolutional-networks/#conv" >http://cs231n.github.io/convolutional-networks/#conv<i class="fas fa-external-link-alt"></i></a>)</em></p>
<p>The reason is surprisingly simple:</p>
<ul>
<li>We have a lot <strong>more data</strong>.</li>
<li>We have <strong>much powerful computers</strong>.</li>
</ul>
<p>A large and deep neural network has many more layers + many more nodes in each layer, which results in exponentially many more parameters to tune. Without enough data, we cannot learn parameters efficiently. Without powerful computers, learning would be too slow and insufficient.</p>
<p>Here is an interesting plot presenting the relationship between the data scale and the model performance, proposed by Andrew Ng in his “<a class="link"   target="_blank" rel="noopener" href="https://youtu.be/F1ka6a13S9I" >Nuts and Bolts of Applying Deep Learning<i class="fas fa-external-link-alt"></i></a>“ talk. On a small dataset, traditional algorithms (Regression, Random Forests, SVM, GBM, etc.) or statistical learning does a great job, but once the data scale goes up to the sky, the large NN outperforms others. Partially because compared to a traditional ML model, a neural network model has many more parameters and has the capability to learn complicated nonlinear patterns. Thus we expect the model to pick the most helpful features by itself without too much expert-involved manual feature engineering.</p>
<p><img src="!--swig%EF%BF%BC1--" alt="Data size versus model performance"><br>{: style=”width: 400px; max-width: 100%;”}<br><em>Fig 2: The data scale versus the model performance. (Recreated based on: <a class="link"   target="_blank" rel="noopener" href="https://youtu.be/F1ka6a13S9I" >https://youtu.be/F1ka6a13S9I<i class="fas fa-external-link-alt"></i></a>)</em></p>
<h2 id="Deep-Learning-Models"><a href="#Deep-Learning-Models" class="headerlink" title="Deep Learning Models"></a>Deep Learning Models</h2><p>Next, let’s go through a few classical deep learning models. </p>
<h3 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h3><p>Convolutional neural networks, short for “CNN”, is a type of feed-forward artificial neural networks, in which the connectivity pattern between its neurons is inspired by the organization of the visual cortex system. The primary visual cortex (V1) does edge detection out of the raw visual input from the retina. The secondary visual cortex (V2), also called prestriate cortex, receives the edge features from V1 and extracts simple visual properties such as orientation, spatial frequency, and color. The visual area V4 handles more complicated object attributes. All the processed visual features flow into the final logic unit, inferior temporal gyrus (IT), for object recognition. The shortcut between V1 and V4 inspires a special type of CNN with connections between non-adjacent layers: Residual Net (<a class="link"   target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" >He, et al. 2016<i class="fas fa-external-link-alt"></i></a>) containing “Residual Block” which supports some input of one layer to be passed to the component two layers later. </p>
<p><img src="!--swig%EF%BF%BC2--" alt="Human visual cortex system"><br>{: style=”width: 680px; max-width: 100%;”}<br><em>Fig 3: Illustration of the human visual cortex system. (The source of the left image: Wang, Haohan, Bhiksha Raj, and Eric P. Xing. <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/1702.07800.pdf" >“On the Origin of Deep Learning.”<i class="fas fa-external-link-alt"></i></a> arXiv preprint arXiv:1702.07800, 2017.)</em></p>
<p>Convolution is a mathematical term, here referring to an operation between two matrices. The convolutional layer has a fixed small matrix defined, also called kernel or filter. As the kernel is sliding, or convolving, across the matrix representation of the input image, it is computing the element-wise multiplication of the values in the kernel matrix and the original image values. <a class="link"   target="_blank" rel="noopener" href="http://setosa.io/ev/image-kernels/" >Specially designed kernels<i class="fas fa-external-link-alt"></i></a> can process images for common purposes like blurring, sharpening, edge detection and many others, fast and efficiently.</p>
<p><img src="!--swig%EF%BF%BC3--" alt="Architecture of LeNet"><br>{: style=”padding-bottom: 3px;”}<br><em>Fig 4: The LeNet architecture consists of two sets of convolutional, activation, and pooling layers, followed by a fully-connected layer, activation, another fully-connected layer, and finally a softmax classifier (Image source: <a class="link"   target="_blank" rel="noopener" href="http://deeplearning.net/tutorial/lenet.html" >http://deeplearning.net/tutorial/lenet.html<i class="fas fa-external-link-alt"></i></a>)</em></p>
<p><a class="link"   target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/" >Convolutional<i class="fas fa-external-link-alt"></i></a> and <a class="link"   target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/supervised/Pooling/" >pooling<i class="fas fa-external-link-alt"></i></a> (or “sub-sampling” in Fig. 4) layers act like the V1, V2 and V4 visual cortex units, responding to feature extraction. The object recognition reasoning happens in the later fully-connected layers which consume the extracted features.</p>
<h3 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h3><p>A sequence model is usually designed to transform an input sequence into an output sequence that lives in a different domain. Recurrent neural network, short for “RNN”, is suitable for this purpose and has shown tremendous improvement in problems like handwriting recognition, speech recognition, and machine translation (<a class="link"   target="_blank" rel="noopener" href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf" >Sutskever et al. 2011<i class="fas fa-external-link-alt"></i></a>, <a class="link"   target="_blank" rel="noopener" href="http://www6.in.tum.de/Main/Publications/Liwicki2007a.pdf" >Liwicki et al. 2007<i class="fas fa-external-link-alt"></i></a>).</p>
<p>A recurrent neural network model is born with the capability to process long sequential data and to tackle tasks with context spreading in time. The model processes one element in the sequence at one time step. After computation, the newly updated unit state is passed down to the next time step to facilitate the computation of the next element. Imagine the case when an RNN model reads all the Wikipedia articles, character by character, and then it can predict the following words given the context.</p>
<p><img src="!--swig%EF%BF%BC4--" alt="Recurrent neural network"><br>{: style=”width: 500px; max-width: 100%;”}<br><em>Fig 5. A recurrent neural network with one hidden unit (left) and its unrolling version in time (right). The unrolling version illustrates what happens in time: $$s_{t-1}$$, $$s_{t}$$, and $$s_{t+1}$$ are the same unit with different states at different time steps $$t-1$$, $$t$$, and $$t+1$$. (Image source: <a class="link"   target="_blank" rel="noopener" href="http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf" >LeCun, Bengio, and Hinton, 2015<i class="fas fa-external-link-alt"></i></a>; <a class="link"   target="_blank" rel="noopener" href="https://www.nature.com/nature/journal/v521/n7553/fig_tab/nature14539_F5.html" >Fig. 5<i class="fas fa-external-link-alt"></i></a>)</em></p>
<p>However, simple perceptron neurons that linearly combine the current input element and the last unit state may easily lose the long-term dependencies. For example, we start a sentence with “Alice is working at …” and later after a whole paragraph, we want to start the next sentence with “She” or “He” correctly. If the model forgets the character’s name “Alice”, we can never know. To resolve the issue, researchers created a special neuron with a much more complicated internal structure for memorizing long-term context, named <a class="link"   target="_blank" rel="noopener" href="http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf" >“Long-short term memory (LSTM)”<i class="fas fa-external-link-alt"></i></a> cell. It is smart enough to learn for how long it should memorize the old information, when to forget, when to make use of the new data, and how to combine the old memory with new input. This <a class="link"   target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" >introduction<i class="fas fa-external-link-alt"></i></a> is so well written that I recommend everyone with interest in LSTM to read it. It has been officially promoted in the <a class="link"   target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/recurrent" >Tensorflow documentation<i class="fas fa-external-link-alt"></i></a> ;-)</p>
<p><img src="!--swig%EF%BF%BC5--" alt="LSTM"><br>{: style=”width: 320px; max-width: 100%;”}<br><em>Fig 6. The structure of a LSTM cell. (Image source: <a class="link"   target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" >http://colah.github.io/posts/2015-08-Understanding-LSTMs/<i class="fas fa-external-link-alt"></i></a>)</em></p>
<p>To demonstrate the power of RNNs, <a class="link"   target="_blank" rel="noopener" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" >Andrej Karpathy<i class="fas fa-external-link-alt"></i></a> built a character-based language model using RNN with LSTM cells.  Without knowing any English vocabulary beforehand, the model could learn the relationship between characters to form words and then the relationship between words to form sentences. It could achieve a decent performance even without a huge set of training data.</p>
<p><img src="!--swig%EF%BF%BC6--" alt="Shakespeare by RNN"><br>{: style=”width: 500px”}<br><em>Fig 7. A character-based recurrent neural network model writes like a Shakespeare. (Image source: <a class="link"   target="_blank" rel="noopener" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" >http://karpathy.github.io/2015/05/21/rnn-effectiveness/<i class="fas fa-external-link-alt"></i></a>)</em></p>
<h3 id="RNN-Sequence-to-Sequence-Model"><a href="#RNN-Sequence-to-Sequence-Model" class="headerlink" title="RNN: Sequence-to-Sequence Model"></a>RNN: Sequence-to-Sequence Model</h3><p>The <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.1078.pdf" >sequence-to-sequence model<i class="fas fa-external-link-alt"></i></a> is an extended version of RNN, but its application field is distinguishable enough that I would like to list it in a separated section. Same as RNN, a sequence-to-sequence model operates on sequential data, but particularly it is commonly used to develop chatbots or personal assistants, both generating meaningful response for input questions. A sequence-to-sequence model consists of two RNNs, encoder and decoder. The encoder learns the contextual information from the input words and then hands over the knowledge to the decoder side through a “<strong>context vector</strong>“ (or “thought vector”, as shown in Fig 8.). Finally, the decoder consumes the context vector and generates proper responses.</p>
<p><img src="!--swig%EF%BF%BC7--" alt="Sequence-to-sequence model"><br>{: }<br><em>Fig 8. A sequence-to-sequence model for generating Gmail auto replies. (Image source: <a class="link"   target="_blank" rel="noopener" href="https://research.googleblog.com/2015/11/computer-respond-to-this-email.html" >https://research.googleblog.com/2015/11/computer-respond-to-this-email.html<i class="fas fa-external-link-alt"></i></a>)</em></p>
<h3 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a>Autoencoders</h3><p>Different from the previous models, autoencoders are for unsupervised learning. It is designed to learn a <strong>low-dimensional</strong> representation of a <strong>high-dimensional</strong> data set, similar to what <a class="link"   target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Principal_component_analysis" >Principal Components Analysis (PCA)<i class="fas fa-external-link-alt"></i></a> does. The autoencoder model tries to learn an approximation function $$ f(x) \approx x $$ to reproduce the input data. However, it is restricted by a bottleneck layer in the middle with a very small number of nodes. With limited capacity, the model is forced to form a very efficient encoding of the data, that is essentially the low-dimensional code we learned.</p>
<p><img src="!--swig%EF%BF%BC8--" alt="Autoencoder"><br>{: style=”width: 300px; max-width: 100%;”}<br><em>Fig 9. An autoencoder model has a bottleneck layer with only a few neurons. (Image source: Geoffrey Hinton’s Coursera class <a class="link"   target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks" >“Neural Networks for Machine Learning”<i class="fas fa-external-link-alt"></i></a> - <a class="link"   target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks/home/week/15" >Week 15<i class="fas fa-external-link-alt"></i></a>)</em></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/7d76/b71b700846901ac4ac119403aa737a285e36.pdf" >Hinton and Salakhutdinov<i class="fas fa-external-link-alt"></i></a> used autoencoders to compress documents on a variety of topics. As shown in Fig 10, when both PCA and autoencoder were applied to reduce the documents onto two dimensions, autoencoder demonstrated a much better outcome. With the help of autoencoder, we can do efficient data compression to speed up the information retrieval including both documents and images.</p>
<p><img src="!--swig%EF%BF%BC9--" alt="Autoencoder experiment"><br><em>Fig 10. The outputs of PCA (left) and autoencoder (right) when both try to compress documents into two numbers. (Image source: Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. <a class="link"   target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/7d76/b71b700846901ac4ac119403aa737a285e36.pdf" >“Reducing the dimensionality of data with neural networks.”<i class="fas fa-external-link-alt"></i></a> science 313.5786 (2006): 504-507.)</em></p>
<h2 id="Reinforcement-Deep-Learning"><a href="#Reinforcement-Deep-Learning" class="headerlink" title="Reinforcement (Deep) Learning"></a>Reinforcement (Deep) Learning</h2><p>Since I started my post with AlphaGo, let us dig a bit more on why AlphaGo worked out. <a class="link"   target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Reinforcement_learning" >Reinforcement learning (“RL”)<i class="fas fa-external-link-alt"></i></a> is one of the secrets behind its success. RL is a subfield of machine learning which allows machines and software agents to automatically determine the optimal behavior within a given context, with a goal to maximize the long-term performance measured by a given metric.</p>
<p><img src="!--swig%EF%BF%BC10--" alt="AlphaGo paper"></p>
<p><img src="!--swig%EF%BF%BC11--" alt="AlphaGo model"><br>{: style=”width: 600px; max-width: 100%;”}<br><em>Fig 11. AlphaGo neural network training pipeline and architecture. (Image source: Silver, David, et al. <a class="link"   target="_blank" rel="noopener" href="http://web.iitd.ac.in/~sumeet/Silver16.pdf" >“Mastering the game of Go with deep neural networks and tree search.”<i class="fas fa-external-link-alt"></i></a> Nature 529.7587 (2016): 484-489.)</em></p>
<p>The AlphaGo system starts with a supervised learning process to train a fast rollout policy and a policy network, relying on the manually curated training dataset of professional players’ games. It learns what is the best strategy given the current position on the game board. Then it applies reinforcement learning by setting up self-play games. The RL policy network gets improved when it wins more and more games against previous versions of the policy network. In the self-play stage, AlphaGo becomes stronger and stronger by playing against itself without requiring additional external training data.</p>
<h3 id="Generative-Adversarial-Network"><a href="#Generative-Adversarial-Network" class="headerlink" title="Generative Adversarial Network"></a>Generative Adversarial Network</h3><p><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.2661.pdf" >Generative adversarial network<i class="fas fa-external-link-alt"></i></a>, short for “GAN”, is a type of deep generative models. GAN is able to create new examples after learning through the real data.  It is consist of two models competing against each other in a zero-sum game framework. The famous deep learning researcher <a class="link"   target="_blank" rel="noopener" href="http://yann.lecun.com/" >Yann LeCun<i class="fas fa-external-link-alt"></i></a> gave it a super high praise: Generative Adversarial Network is the most interesting idea in the last ten years in machine learning. (See the Quora question: <a class="link"   target="_blank" rel="noopener" href="https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning" >“What are some recent and potentially upcoming breakthroughs in deep learning?”<i class="fas fa-external-link-alt"></i></a>)</p>
<p><img src="!--swig%EF%BF%BC12--" alt="Generative adversarial network"><br>{: style=”width: 600px; max-width: 100%;”}<br><em>Fig. 12. The architecture of a generative adversarial network. (Image source: <a class="link"   target="_blank" rel="noopener" href="http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html" >http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html<i class="fas fa-external-link-alt"></i></a>)</em></p>
<p>In the <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.2661.pdf" >original GAN paper<i class="fas fa-external-link-alt"></i></a>, GAN was proposed to generate meaningful images after learning from real photos. It comprises two independent models: the <strong>Generator</strong> and the <strong>Discriminator</strong>. The generator produces fake images and sends the output to the discriminator model. The discriminator works like a judge, as it is optimized for identifying the real photos from the fake ones. The generator model is trying hard to cheat the discriminator while the judge is trying hard not to be cheated. This interesting zero-sum game between these two models motivates both to develop their designed skills and improve their functionalities. Eventually, we take the generator model for producing new images.</p>
<h2 id="Toolkits-and-Libraries"><a href="#Toolkits-and-Libraries" class="headerlink" title="Toolkits and Libraries"></a>Toolkits and Libraries</h2><p>After learning all these models, you may start wondering how you can implement the models and use them for real. Fortunately, we have many open source toolkits and libraries for building deep learning models. <a class="link"   target="_blank" rel="noopener" href="https://www.tensorflow.org/" >Tensorflow<i class="fas fa-external-link-alt"></i></a> is fairly new but has attracted a lot of popularity. It turns out, TensorFlow was <a class="link"   target="_blank" rel="noopener" href="http://deliprao.com/archives/168" >the most forked Github project of 2015<i class="fas fa-external-link-alt"></i></a>. All that happened in a period of 2 months after its release in Nov 2015.</p>
<p><img src="!--swig%EF%BF%BC13--" alt="Deep learning toolkits"><br>{: style=”padding-bottom: 15px; max-width: 100%;”}</p>
<h2 id="How-to-Learn"><a href="#How-to-Learn" class="headerlink" title="How to Learn?"></a>How to Learn?</h2><p>If you are very new to the field and willing to devote some time to studying deep learning in a more systematic way, I would recommend you to start with the book <a class="link"   target="_blank" rel="noopener" href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?s=books&ie=UTF8&qid=1499413305&sr=1-1&keywords=deep+learning" >Deep Learning<i class="fas fa-external-link-alt"></i></a> by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. The Coursera course <a class="link"   target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks" >“Neural Networks for Machine Learning”<i class="fas fa-external-link-alt"></i></a> by Geoffrey Hinton (<a class="link"   target="_blank" rel="noopener" href="https://youtu.be/uAu3jQWaN6E" >Godfather of deep learning!<i class="fas fa-external-link-alt"></i></a>). The content for the course was prepared around 2006, pretty old, but it helps you build up a solid foundation for understanding deep learning models and expedite further exploration.</p>
<p>Meanwhile, maintain your curiosity and passion. The field is making progress every day. Even classical or widely adopted deep learning models may just have been proposed 1-2 years ago. Reading academic papers can help you learn stuff in depth and keep up with the cutting-edge findings.</p>
<h4 id="Useful-resources"><a href="#Useful-resources" class="headerlink" title="Useful resources"></a>Useful resources</h4><ul>
<li>Google Scholar: <a class="link"   target="_blank" rel="noopener" href="http://scholar.google.com/" >http://scholar.google.com<i class="fas fa-external-link-alt"></i></a></li>
<li>arXiv cs section: <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/list/cs/recent" >https://arxiv.org/list/cs/recent<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="http://ufldl.stanford.edu/tutorial/" >Unsupervised Feature Learning and Deep Learning Tutorial<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/" >Tensorflow Tutorials<i class="fas fa-external-link-alt"></i></a></li>
<li>Data Science Weekly</li>
<li><a class="link"   target="_blank" rel="noopener" href="http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html" >KDnuggets<i class="fas fa-external-link-alt"></i></a></li>
<li>Tons of blog posts and online tutorials</li>
<li>Related <a class="link"   target="_blank" rel="noopener" href="http://coursera.com/" >Cousera<i class="fas fa-external-link-alt"></i></a> courses</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://github.com/terryum/awesome-deep-learning-papers" >awesome-deep-learning-papers<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<h4 id="Blog-posts-mentioned"><a href="#Blog-posts-mentioned" class="headerlink" title="Blog posts mentioned"></a>Blog posts mentioned</h4><ul>
<li><a class="link"   target="_blank" rel="noopener" href="http://setosa.io/ev/image-kernels" >Explained Visually: Image Kernels<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" >Understanding LSTM Networks<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" >The Unreasonable Effectiveness of Recurrent Neural Networks<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://research.googleblog.com/2015/11/computer-respond-to-this-email.html" >Computer, respond to this email.<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<h4 id="Interesting-blogs-worthy-of-checking"><a href="#Interesting-blogs-worthy-of-checking" class="headerlink" title="Interesting blogs worthy of checking"></a>Interesting blogs worthy of checking</h4><ul>
<li><a class="link"   target="_blank" rel="noopener" href="http://www.wildml.com/" >www.wildml.com<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="http://colah.github.io/" >colah.github.io<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="http://karpathy.github.io/" >karpathy.github.io<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://blog.openai.com/" >blog.openai.com<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<h4 id="Papers-mentioned"><a href="#Papers-mentioned" class="headerlink" title="Papers mentioned"></a>Papers mentioned</h4><p>[1] He, Kaiming, et al. <a class="link"   target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" >“Deep residual learning for image recognition.”<i class="fas fa-external-link-alt"></i></a> Proc. IEEE Conf. on computer vision and pattern recognition. 2016.</p>
<p>[2] Wang, Haohan, Bhiksha Raj, and Eric P. Xing. <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/1702.07800.pdf" >“On the Origin of Deep Learning.”<i class="fas fa-external-link-alt"></i></a> arXiv preprint arXiv:1702.07800, 2017.</p>
<p>[3] Sutskever, Ilya, James Martens, and Geoffrey E. Hinton. <a class="link"   target="_blank" rel="noopener" href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf" >“Generating text with recurrent neural networks.”<i class="fas fa-external-link-alt"></i></a> Proc. of the 28th Intl. Conf. on Machine Learning (ICML). 2011. </p>
<p>[4] Liwicki, Marcus, et al. <a class="link"   target="_blank" rel="noopener" href="http://www6.in.tum.de/Main/Publications/Liwicki2007a.pdf" >“A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks.”<i class="fas fa-external-link-alt"></i></a> Proc. of 9th Intl. Conf. on Document Analysis and Recognition. 2007.</p>
<p>[5] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. <a class="link"   target="_blank" rel="noopener" href="http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf" >“Deep learning.”<i class="fas fa-external-link-alt"></i></a> Nature 521.7553 (2015): 436-444.</p>
<p>[6] Hochreiter, Sepp, and Jurgen Schmidhuber. <a class="link"   target="_blank" rel="noopener" href="http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf" >“Long short-term memory.”<i class="fas fa-external-link-alt"></i></a> Neural computation 9.8 (1997): 1735-1780.</p>
<p>[7] Cho, Kyunghyun. et al. <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.1078.pdf" >“Learning phrase representations using RNN encoder-decoder for statistical machine translation.”<i class="fas fa-external-link-alt"></i></a> Proc. Conference on Empirical Methods in Natural Language Processing 1724–1734 (2014).</p>
<p>[8] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. <a class="link"   target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/7d76/b71b700846901ac4ac119403aa737a285e36.pdf" >“Reducing the dimensionality of data with neural networks.”<i class="fas fa-external-link-alt"></i></a> science 313.5786 (2006): 504-507.</p>
<p>[9] Silver, David, et al. <a class="link"   target="_blank" rel="noopener" href="http://web.iitd.ac.in/~sumeet/Silver16.pdf" >“Mastering the game of Go with deep neural networks and tree search.”<i class="fas fa-external-link-alt"></i></a> Nature 529.7587 (2016): 484-489.</p>
<p>[10] Goodfellow, Ian, et al. <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.2661.pdf" >“Generative adversarial nets.”<i class="fas fa-external-link-alt"></i></a> NIPS, 2014.</p>
<hr>
<p><em>If you notice mistakes and errors in this post, don’t hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be super happy to correct them right away!</em></p>

        </div>

        
            <div class="post-copyright-info">
                <div class="article-copyright-info-container">
    <ul>
        <li>Post title：An Overview of Deep Learning for Curious People</li>
        <li>Post author：Lutao Dai</li>
        <li>Create time：2017-06-21 01:09:00</li>
        <li>
            Post link：http://example.com/2017/06/21/2017-06-21-an-overview-of-deep-learning/
        </li>
        <li>
            Copyright Notice：All articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

            </div>
        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2020/04/07/2020-04-07-the-transformer-family/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">The Transformer Family</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2017/03/12/2017-03-12-scribble-the-jekyll-theme/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Scribble, a Jekyll theme</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2021</span> -
            
            2021 <i class="fas fa-heart icon-animate"></i> <a href="/">Lutao Dai</a>
        </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a> | Theme <a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.3.0</a>
        </div>
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-Does-Deep-Learning-Work-Now"><span class="nav-number">1.</span> <span class="nav-text">Why Does Deep Learning Work Now?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Learning-Models"><span class="nav-number">2.</span> <span class="nav-text">Deep Learning Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Convolutional-Neural-Network"><span class="nav-number">2.1.</span> <span class="nav-text">Convolutional Neural Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Recurrent-Neural-Network"><span class="nav-number">2.2.</span> <span class="nav-text">Recurrent Neural Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN-Sequence-to-Sequence-Model"><span class="nav-number">2.3.</span> <span class="nav-text">RNN: Sequence-to-Sequence Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Autoencoders"><span class="nav-number">2.4.</span> <span class="nav-text">Autoencoders</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reinforcement-Deep-Learning"><span class="nav-number">3.</span> <span class="nav-text">Reinforcement (Deep) Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Generative-Adversarial-Network"><span class="nav-number">3.1.</span> <span class="nav-text">Generative Adversarial Network</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Toolkits-and-Libraries"><span class="nav-number">4.</span> <span class="nav-text">Toolkits and Libraries</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-Learn"><span class="nav-number">5.</span> <span class="nav-text">How to Learn?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Useful-resources"><span class="nav-number">5.0.1.</span> <span class="nav-text">Useful resources</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Blog-posts-mentioned"><span class="nav-number">5.0.2.</span> <span class="nav-text">Blog posts mentioned</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interesting-blogs-worthy-of-checking"><span class="nav-number">5.0.3.</span> <span class="nav-text">Interesting blogs worthy of checking</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Papers-mentioned"><span class="nav-number">5.0.4.</span> <span class="nav-text">Papers mentioned</span></a></li></ol></li></ol></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>





<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



</body>
</html>
